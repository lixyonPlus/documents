# Redis

## redis分布式锁： 
### 普通锁：
    ```lua
    - 获取锁（unique_value可以是UUID等）
    SET resource_name unique_value NX PX 30000
    - 释放锁（lua脚本中，一定要比较value，防止误解锁）
    if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
    else
    return 0
    end
    ```
    
#### 这种实现方式有3大要点（也是面试概率非常高的地方）：
    - set命令要用set key value px milliseconds nx；
    - value要具有唯一性；
    - 释放锁时要验证value值，不能误解锁；
    - 事实上这类琐最大的缺点就是它加锁时只作用在一个Redis节点上，即使Redis通过sentinel保证高可用，如果这个master节点由于某些原因发生了主从切换，那么就会出现锁丢失的情况：
        1. 在Redis的master节点上拿到了锁；
        2. 但是这个加锁的key还没有同步到slave节点；
        3. master故障，发生故障转移，slave节点升级为master节点；导致锁丢失。

 ### redLock:
   - 在Redis的分布式环境中，我们假设有N个Redis master。这些节点完全互相独立，不存在主从复制或者其他集群协调机制。我们确保将在N个实例上使用与在Redis单实例下相同方法获取和释放锁。现在我们假设有5个Redis master节点，同时我们需要在5台服务器上面运行这些Redis实例，这样保证他们不会同时都宕掉。为了取到锁，客户端应该执行以下操作:
      1. 获取当前Unix时间，以毫秒为单位。[开始获取锁时间]
      2. 依次尝试从5个实例，使用相同的key和具有唯一性的value（例如UUID）获取锁。当向Redis请求获取锁时，客户端应该设置一个网络连接和响应超时时间，这个超时时间应该小于锁的失效时间。例如你的锁自动失效时间为10秒，则超时时间应该在5-50毫秒之间。这样可以避免服务器端Redis已经挂掉的情况下，客户端还在死死地等待响应结果。如果服务器端没有在规定时间内响应，客户端应该尽快尝试去另外一个Redis实例请求获取锁。
      3. 客户端使用最后一次获取锁时间减去开始获取锁时间（步骤1记录的时间）就得到获取锁使用的时间。当且仅当从大多数（N/2+1，这里是3个节点）的Redis节点都取到锁，并且使用的时间小于锁失效时间时，锁才算获取成功。[获取锁使用时间=最后获取锁时间-开始获取锁时间]
      4. 如果取到了锁，key的真正有效时间等于有效时间减去获取锁所使用的时间。[锁有效时间=有效时间-获取锁使用时间]
      5. 如果因为某些原因，获取锁失败（没有在至少N/2+1个Redis实例取到锁或者取锁时间已经超过了有效时间），客户端应该在所有的Redis实例上进行解锁（即便某些Redis实例根本就没有加锁成功，防止某些节点获取到锁但是客户端没有得到响应而导致接下来的一段时间不能被重新获取锁）。

   - 加锁的流程：（lua：一堆命令保证原子性）
      1. 判断lock键是否存在，不存在直接调用hset存储当前线程信息并且设置过期时间,返回nil，告诉客户端直接获取到锁。
      2. 判断lock键是否存在，存在则将重入次数加1，并重新设置过期时间，返回nil，告诉客户端直接获取到锁。
      3. 被其它线程已经锁定，返回锁有效期的剩余时间，告诉客户端需要等待。
   - 解锁的流程：
      1. 解锁的流程看起来复杂些：
      2. 如果lock键不存在，发消息说锁已经可用
      3. 如果锁不是被当前线程锁定，则返回nil
      4. 由于支持可重入，在解锁时将重入次数需要减1
      5. 如果计算后的重入次数>0，则重新设置过期时间
      6. 如果计算后的重入次数<=0，则发消息说锁已经可用
   redlock缺点：
    1.RedLock中，为了防止死锁，锁是具有过期时间的。如果 Client 1 在持有锁的时候，发生了一次很长时间的 FGC 超过了锁的过期时间。锁就被释放了。这个时候 Client 2 又获得了一把锁，提交数据。这个时候 Client 1 从 FGC 中苏醒过来了，又一次提交数据。RedLock 只是保证了锁的高可用性，并没有保证锁的正确性。
    2.RedLock 是一个 严重依赖系统时钟 的分布式系统。Client 1 从 A、B、D、E五个节点中，获取了 A、B、C三个节点获取到锁，我们认为他持有了锁,这个时候，由于 B 的系统时间比别的系统走得快，B就会先于其他两个节点优先释放锁。Clinet 2 可以从 B、D、E三个节点获取到锁。在整个分布式系统就造成 两个 Client 同时持有锁了。

## 高可用方案：
  1. 主从复制：（缺点：master宕机无法只能主动切换salve）
    Redis的主从复制功能分为两种数据同步模式：全量数据同步和增量数据同步。
    当Slave节点给定的run_id和Master的run_id不一致时，或者Slave给定的上一次增量同步的offset的位置在Master的环形内存中无法定位时（后文会提到），Master就会对Slave发起全量同步操作。
    这时无论您是否在Master打开了RDB快照功能，它和Slave节点的每一次全量同步操作过程都会更新/创建Master上的RDB文件。在Slave连接到Master，并完成第一次全量数据同步后，接下来Master到Slave的数据同步过程一般就是增量同步形式了（也称为部分同步）。增量同步过程不再主要依赖RDB文件，Master会将新产生的数据变化操作存放在一个内存区域，这个内存区域采用环形构造。

  2. 全量同步：
      master启动运行
      slave启动，连接master，发送第一个ping命令，master回应pong，slave收到master回应pong，发送第一次全量同步命令，master接收到同步命令，执行bgsave命令，异步发送完整的rdb文件给slave，salve接收到rdb文件，加载到内存，通知master收到完成通知。
  3. 增量同步：
      salve启动，连接master，发送ping命令，master回复pong，salve收到master回应pong，发送同步命令，（根据run_id或offset判断）确定可进行增量更新，从环形内存中取得增量，发送给slave，salve更新内存，更新rdb或aof（如果需要），收到一个数据更新操作，写aof（如果需要），更新到环形内存，主动更新slave，slave收到更新数据，更新内存，（跟新rdb或aof（如果需要））
      增量更新时，不依赖rdb文件，所以master和slave是否开启rdb功能或者aof不重要。

 #### 为什么在Master上新增的数据除了根据Master节点上RDB或者AOF的设置进行日志文件更新外，还会同时将数据变化写入一个环形内存结构，并以后者为依据进行Slave节点的增量更新呢？主要原因有以下几个：

    1.由于网络环境的不稳定，网络抖动/延迟都可能造成Slave和Master暂时断开连接，这种情况要远远多于新的Slave连接到Master的情况。如果以上所有情况都使用全量更新，就会大大增加Master的负载压力——写RDB文件是有大量I/O过程的，虽然Linux Page Cahe特性会减少性能消耗。
    2.另外在数据量达到一定规模的情况下，使用全量更新进行和Slave的第一次同步是一个不得已的选择——因为要尽快减少Slave节点和Master节点的数据差异。所以只能占用Master节点的资源和网络带宽资源。
    3.使用内存记录数据增量操作，可以有效减少Master节点在这方面付出的I/O代价。而做成环形内存的原因，是为了保证在满足数据记录需求的情况下尽可能减少内存的占用量。这个环形内存的大小，可以通过repl-backlog-size参数进行设置。
    Slave重连后会向Master发送之前接收到的Master run_id信息和上一次完成部分同步的offset的位置信息。如果Master能够确定这个run_id和自己的run_id一致且能够在环形内存中找到这个offset的位置，Master就会发送从offset的位置开始向Slave发送增量数据。那么连接正常的各个Slave节点如何接受新数据呢？连接正常的Slave节点将会在Master节点将数据写入环形内存后，主动接收到来自Master的数据复制信息。
    
  ### 哨兵机制：
    有了主从复制以后，如果想对主服务器进行监控，可以使用哨兵机制。
     a.（监控）监控所有节点数据库是否正常运行。
     b.（自动故障迁移）master故障时，可以通过投票机制，从slave节点中选举新的master。
     c.（提醒）当某个redis出现问题，可以通过api提醒用户。
     
   #### 监控
   - sentinel会每秒一次的频率与之前创建了命令连接的实例发送PING，包括主服务器、从服务器和sentinel实例，以此来判断当前实例的状态。down-after-milliseconds时间内PING连接无效，则将该实例视为主观下线。之后该sentinel会向其他监控同一主服务器的sentinel实例询问是否也将该服务器视为主观下线状态，当超过某quorum后将其视为客观下线状态。当一个主服务器被某sentinel视为客观下线状态后，该sentinel会与其他sentinel协商选出零头sentinel进行故障转移工作。每个发现主服务器进入客观下线的sentinel都可以要求其他sentinel选自己为领头sentinel，选举是先到先得。同时每个sentinel每次选举都会自增配置纪元，每个纪元中只会选择一个领头sentinel。如果所有超过一半的sentinel选举某sentinel领头sentinel。之后该sentinel进行故障转移操作。如果一个Sentinel为了指定的主服务器故障转移而投票给另一个Sentinel，将会等待一段时间后试图再次故障转移这台主服务器。如果该次失败另一个将尝试，Redis Sentinel保证第一个活性(liveness)属性，如果大多数Sentinel能够对话，如果主服务器下线，最后只会有一个被授权来故障转移。 同时Redis Sentinel也保证安全(safety)属性，每个Sentinel将会使用不同的配置纪元来故障转移同一台主服务器。
  
   #### 故障迁移
   - 首先是从主服务器的从服务器中选出一个从服务器作为新的主服务器。选点的依据依次是：网络连接正常->5秒内回复过INFO命令->10*down-after-milliseconds内与主连接过的->从服务器优先级->复制偏移量->运行id较小的。选出之后通过slaveif no ont将该从服务器升为新主服务器。通过slaveof ip port命令让其他从服务器复制该信主服务器。最后当旧主重新连接后将其变为新主的从服务器。注意如果客户端与就主服务器分隔在一起，写入的数据在恢复后由于旧主会复制新主的数据会造成数据丢失。故障转移成功后会通过发布订阅连接广播新的配置信息，其他sentinel收到后依据配置纪元更大来更新主服务器信息。Sentinel保证第二个活性属性：一个可以相互通信的Sentinel集合会统一到一个拥有更高版本号的相同配置上。缺点：
    1. 主从服务器的数据要经常进行主从复制，这样造成性能下降。
    2. 当主服务器宕机后，从服务器切换成主服务器的那段时间，服务是不能用的。
    
  ### Redis Cluster：
        Redis集群是一个分布式（distributed）、容错（fault-tolerant）的 Redis内存K/V服务， 集群可以使用的功能是普通单机 Redis 所能使用的功能的一个子集（subset），比如Redis集群并不支持处理多个keys的命令,因为这需要在不同的节点间移动数据,从而达不到像Redis那样的性能,在高负载的情况下可能会导致不可预料的错误。
        
   #### Redis集群的几个重要特征：
    　　(1). Redis 集群的分片特征在于将键空间分拆了16384个槽位，每一个节点负责其中一些槽位。
    　　(2). Redis提供一定程度的可用性,可以在某个节点宕机或者不可达的情况下继续处理命令.
    　　(3). Redis 集群中不存在中心（central）节点或者代理（proxy）节点， 集群的其中一个主要设计目标是达到线性可扩展性.
      Redis Cluster特点如下：
        所有的节点相互连接；
        集群消息通信通过集群总线通信，集群总线端口大小为客户端服务端口+10000，这个10000是固定值；
        节点与节点之间通过二进制协议进行通信；
        客户端和集群节点之间通信和通常一样，通过文本协议进行；
        集群节点不会代理查询；
        
   ##### 分区实现原理
   - 1.槽（slot）概念: Redis Cluster中有一个16384长度的槽的概念，他们的编号为0、1、2、3……16382、16383。这个槽是一个虚拟的槽，并不是真正存在的。正常工作的时候，Redis Cluster中的每个Master节点都会负责一部分的槽，当有某个key被映射到某个Master负责的槽，那么这个Master负责为这个key提供服务，至于哪个Master节点负责哪个槽，这是可以由用户指定的，也可以在初始化的时候自动生成（redis-trib.rb脚本）。这里值得一提的是，在Redis Cluster中，只有Master才拥有槽的所有权，如果是某个Master的slave，这个slave只负责槽的使用，但是没有所有权。Redis Cluster怎么知道哪些槽是由哪些节点负责的呢？某个Master又怎么知道某个槽自己是不是拥有呢？
   - 2.位序列结构,Master节点维护着一个16384/8字节的位序列，Master节点用bit来标识对于某个槽自己是否拥有。比如对于编号为1的槽，Master只要判断序列的第二位（索引从0开始）是不是为1即可。
   - 3.故障容忍度
    - 1. 心跳和gossip消息,Redis Cluster持续的交换PING和PONG数据包。这两种数据包的数据结构相同，都包含重要的配置信息，唯一的不同是消息类型字段。PING和PONG数据包统称为心跳数据包。每个节点在每一秒钟都向一定数量的其它节点发送PING消息，这些节点应该向发送PING的节点回复一个PONG消息。节点会尽可能确保拥有每个其它节点在NOTE_TIMEOUT/2秒时间内的最新信息，否则会发送一个PING消息，以确定与该节点的连接是否正常。假定一个Cluster有301个节点，NOTE_TIMEOUT为60秒，那么每30秒每个节点至少发送300个PING，即每秒10个PING， 整个Cluster每秒发送10x301=3010个PING。这个数量级的流量不应该会造成网络负担。
    - 2. 故障检测,Redis Cluster的故障检测用于检测一个master节点何时变得不再有效，即不能提供服务，从而应该让slave节点提升为master节点。如果提升失败，则整个Cluster失效，不再接受客户端的服务请求。当一个节点A向另外一个节点B发送了PING消息之后，经过NODE_TIMEOUT秒时间之后仍然没有收到PONG应答，则节点A认为节点B失效，节点A将为该节点B设置PFAIL标志。在 NODE_TIMEOUT * FAIL_REPORT_VALIDITY_MULT时间内，当Cluster中大多数节点认为节点B失效，即设置PFAIL标志时，这个Cluster认为节点B真的失效了，此时节点A将为节点B设置FAIL标志，并向所有节点发送FAIL消息。在一些特定情况下，拥有FAIL标志的节点，也可以清除掉FAIL标志。Redis Cluster故障检测机制最终应该让所有节点都一致同意某个节点处于某个确定的状态。如果发生这样的情况少数节点确信某个节点为FAIL，同时有少数节点确认某个节点为非FAIL，则Redis Cluster最终会处于一个确定的状态：
     - 1. 情况1：最终大多数节点认为该节点FAIL，该节点最终实际为FAIL。
     - 2. 情况2：最终在N x NODE_TIMEOUT时间内，仍然只有少数节点将给节点标记为FAIL，此时最终会清除这个节点的FAIL标志。
   - 4.重定向客户端
     - Redis Cluster并不会代理查询，那么如果客户端访问了一个key并不存在的节点，这个节点是怎么处理的呢？比如我想获取key为msg的值，msg计算出来的槽编号为254，当前节点正好不负责编号为254的槽，那么就会返回客户端下面信息：表示客户端想要的254槽由运行在IP为127.0.0.1，端口为6381的Master实例服务。如果根据key计算得出的槽恰好由当前节点负责，则当期节点会立即返回结果。这里明确一下，没有代理的Redis Cluster可能会导致客户端两次连接急群中的节点才能找到正确的服务，推荐客户端缓存连接，这样最坏的情况是两次往返通信。
   - 5.slots配置传播
　   - Redis Cluster采用两种方式进行各个master节点的slots配置信息的传播。所谓slots配置信息，即master负责存储哪几个slots。架构细节:
      - 1. 所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽.
      - 2. 节点的fail是通过集群中超过半数的节点检测失效时才生效.
      - 3. 客户端与redis节点直连,不需要中间proxy层.客户端不需要连接集群所有节点,连接集群中任何一个可用节点即可
      - 4. redis-cluster把所有的物理节点映射到[0-16383]slot上,cluster 负责维护node<->slot<->value
  - Redis 集群中内置了 16384 个哈希槽，当需要在 Redis 集群中放置一个 key-value 时，redis 先对 key 使用 crc16 算法算出一个结果，然后把结果对 16384 求余数，这样每个 key 都会对应一个编号在 0-16383 之间的哈希槽，redis 会根据节点数量大致均等的将哈希槽映射到不同的节点
          
          
          
          
   ### codis：
        codis由3大组件构成：
          codis-server：修改过源码的redis，支持slot、扩容迁移等
          codis-proxy：支持多线程，go语言实现的内核
          codis Dashboard：集群管理工具
        codis提供web图形界面管理集群。
        集群元数据存在在zookeeper或etcd。
        提供独立的组件codis-ha负责redis节点主备切换。
        基于proxy的codis，客户端对路由表变化无感知。客户端需要从codis dashhoard调用list proxy命令获取所有proxy列表，并根据自身的轮询策略决定访问哪个proxy节点以实现负载均衡。




# redis应用场景： 数据缓存，分布式session，分布式锁。

### 常用的淘汰算法：
 - FIFO：First In First Out，先进先出。判断被存储的时间，离目前最远的数据优先被淘汰。
 - LRU：Least Recently Used，最近最少使用。判断最近被使用的时间，目前最远的数据优先被淘汰。
 - LFU：Least Frequently Used，最不经常使用。在一段时间内，数据被使用次数最少的，优先被淘汰。

### Redis提供的淘汰策略：
- noeviction：达到内存限额后返回错误，客户尝试可以导致更多内存使用的命令（大部分写命令，但DEL和一些例外）
- allkeys-lru：为了给新增加的数据腾出空间，驱逐键先试图移除一部分最近使用较少的（LRC）。
- volatile-lru：为了给新增加的数据腾出空间，驱逐键先试图移除一部分最近使用较少的（LRC），但只限于过期设置键。
- allkeys-random: 为了给新增加的数据腾出空间，驱逐任意键
- volatile-random: 为了给新增加的数据腾出空间，驱逐任意键，但只限于有过期设置的驱逐键。
- volatile-ttl: 只限于设置了 expire 的部分; 优先删除剩余时间(time to live,TTL) 短的key。

### redis持久化：
- 1. Redis 默认开启RDB持久化方式，在指定的时间间隔内，执行指定次数的写操作，则将内存中的数据写入到磁盘中。
- 2. RDB 持久化适合大规模的数据恢复但它的数据一致性和完整性较差。
- 3. Redis 需要手动开启AOF持久化方式，默认是每秒将写操作日志追加到AOF文件中。
- 4. AOF 的数据完整性比RDB高，但记录内容多了，会影响数据恢复的效率。
- 5. Redis 针对 AOF文件大的问题，提供重写的瘦身机制。
- 6. 若只打算用Redis 做缓存，可以关闭持久化。
- 7. 若打算使用Redis 的持久化。建议RDB和AOF都开启。其实RDB更适合做数据的备份，留一后手。AOF出问题了，还有RDB。


