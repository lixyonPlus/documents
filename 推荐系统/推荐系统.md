

# 推荐算法
![概括图](https://s1.ax1x.com/2020/10/14/05OiKP.md.jpg)

### 协同过滤是⼀一个⽐较大的算法范畴。通常划分为两类:
1. 基于记忆的协同过滤(Memory-Based)
2. 基于模型的协同过滤(Model-Based )
- 基于记忆的协同过滤，现在看上去极其简单，就是记住每个⼈人消费过什什么东⻄西，然后给他推荐相似的东⻄西，或者推荐相似的人消费的东⻄。
- 基于模型的协同过滤则是从⽤户物品关系矩阵中去学习⼀个模型，从⽽把那些矩阵空⽩处填满。
- 协同过滤最最依赖的是⽤户物品的关系矩阵

---

### User-Based基于用户的协同过滤
基于⽤户的协同过滤⾸先计算相似⽤户，然后再根据相似用户的喜好推荐物品
缺点:
1. ⽤户数量往往⽐较大，计算起来非常吃力，成为瓶颈;
2. 用户的口味其实变化还是很快的，不是静态的，所以兴趣迁移问题很难反应出来;
3. 数据稀疏，⽤户和⽤户之间有共同的消费行为实际上是⽐较少的，⽽且一般都是⼀些热门物品，对发现⽤户兴趣帮助也不大。

### User-Based基于用户的协同过滤有两个产出:
- 相似用户列表
- 基于用户推荐的结果

### 两两计算⽤用户相似度遇到用户量很⼤怎么办?
- 将相似度计算拆成Map Reduce任务，将原始矩阵Map成键为用户对，值为两个⽤户对同一个物品的评分之积，Reduce阶段对这些乘积再求和,Map Reduce任务结束后再对这些值归⼀化

### 相似度计算本身如果遇到超⼤大维度向量量怎么办?
  单个相似度计算问题，如果碰上向量很⻓，⽆论什么相似度计算方法，都要遍历向量，如果⽤循环实现就更可观了，所以通常降低相似度计算复杂度:
  - 1.对向量采样计算。道理很简单，两个⼀百维的向量计算出的相似度是0.7，我现在忍受⼀些精度的损失，不用100维计算，随机从中取出10维计算得到相似度是0.72，显然⽤100维计算出的0.7更可信一些，但是在计算复杂度降低十倍的情形下，0.72 和它误差也不大，后者更经济。这个算法由Twitter提出，叫做DIMSUM算法已经在Spark中实现了
  - 2.向量化计算。与其说这是一个小技巧，不如说这是⼀种思维⽅式。在机器学习领域，向量之间的计算是家常便饭，难道向量计算都要⽤循环实现吗?并不是， 现代的线性代数库都支持直接的向量运算，⽐循环快很多。也就是我们在任何地方，都要想办法把循环转换成向量来直接计算，一般像常用的向量库都天然支持的，比如Python的NumPy

---

### Item-Based基于物品的协同过滤(eg:看过该商品的用户还看了/喜欢该用户的还喜欢了)
基于物品的协同过滤首先计算相似物品，然后再根据用户消费过、或者正在消费的物品为其推荐相似的
- 与基于用户的协同过滤算法比较:
 1. 可以推荐的物品数量往往少于⽤户数量,所以一般计算物品之间的相似度就不会成为瓶颈
 2. 物品之间的相似度比较静态，它们变化的速度没有用户的⼝味变化快,所以完全解耦了用户兴趣迁移这个问题
 3. 物品对应的消费者数量较大，对于计算物品之间的相似度稀疏度是好过计算⽤户之间相似度的
步骤:
- 构建⽤户物品的关系矩阵，矩阵元素可以是用户的消费行为，也可以是消费后的评价，还可以是对消费行为的某种量化如时间、次数、费用等;
- 假如矩阵的行表示物品，列表示⽤户的话，那么就两两计算行向量之间的相似度，得到物品相似度矩阵，行和列都是物品;
- 产⽣推荐结果，根据推荐场景不同，有两种产⽣结果的形式。一种是为某⼀个物品推荐相关物品，另⼀种是在个人⾸页产生类似“猜你喜欢”的推荐结果。
  
- 1.TopK推荐，属于类似“猜你喜欢”这样的。 要预测⼀个用户u对一个物品i的分数，遍历⽤户u评分过的所有物品，假如一共有m个，每⼀个物品和待计算物品i的相似度乘以⽤户的评分，这样加权求和后除以所有这些相似度总和，就得到了一个加权平均评分，作为用户u对物品i的分数预测
- 2.相关推荐，属于"看了⼜看/买了⼜买"

### Slope One算法


### 协同过滤中的相识度算法 
#### 欧氏距离: 如名字所料，是⼀个欧式空间下度量距离的方法。两个物体都在同一个空间下表示为两个点，假如叫做p和q，分别都是n个坐标。那么欧式距离就是衡量这两个点之间的距离，从p到q移动要经过的距离。欧式距离不适合布尔向量之间。

#### 余弦相似度: 度量的是两个向量之间的夹角，其实就是⽤夹⻆的余弦值来度量，所以名字叫余弦相似度。当两个向量的夹⻆为0度时，余弦值为1，当夹角为90度时，余弦值为0，为180度时，余弦值则为-1。余弦相似度在度量文本相似度、用户相似度、物品相似度的时候都较为常用;但是在这里需要提醒你一点，余弦相似度的特点:它与向量的⻓度无关。因为余弦相似度计算需要对向量⻓度做归⼀化,经过向量⻓度归一化后的相似度量⽅式，背后潜藏着这样一种思想:两个向量，只要⽅向一致，无论程度强弱，都可以视为“相似”。
$\color{red}{余弦相似度对绝对值大⼩不敏感这件事，在某些应⽤用上仍然有些问题。
举个例子，⽤户A对两部电影评分分别是1分和2分，⽤户B对同样这两部电影评分是4分和5分。用余弦相似度计算出来，两个用户的相似度达到0.98。这和实际直觉不符，⽤户A明显不喜欢这两部电影。这种情况需要使用调整的余弦相似度算法}$

#### 调整的余弦相似度:调整的⽅法很简单，就是先计算向量每个维度上的均值，然后每个向量在各个维度上都减去均值后，再计算余弦相似度。

#### 皮尔逊相关度: 实际上也是一种余弦相似度，不过先对向量做了中心化，向量p和q各自减去向量的均值后，再计算余弦相似度。
⽪尔逊相关度计算结果范围在-1到1。-1表示负相关，1表示正相关。⽪尔逊相关度其实度量的是两个随机变量是不是在同增同减。

####  杰卡德(Jaccard)相似度: 是两个集合的交集元素个数在并集中所占的⽐例。由于集合⾮常适⽤于布尔向量表示，所以杰卡德相似度简直就是为布尔值向量私人定做的。

#### 余弦相似度适用于评分数据，杰卡德相似度适合⽤于隐式反馈数据。
例如，使⽤用户的收藏行为，计算⽤户之间的相似度，杰卡德相似度就适合来承担这个任务。













