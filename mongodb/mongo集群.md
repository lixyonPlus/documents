### mongo复制集
 复制集的作用
- 在实现高可用的同时，复制集实现了其他几个附加作用:
- 数据分发:将数据从一个区域复制到另一个区域，减少另一个区域的读延迟 
- 读写分离:不同类型的压力分别在不同的节点上执行
- 异地容灾:在数据中心故障时候快速切换到异地

### 典型复制集结构
一个典型的复制集由3个以上具有投票权的节点组成，包括:
- 一个主节点(PRIMARY):接受写入操作和选举时投票
- 两个(或多个)从节点(SECONDARY):复制主节点上的新数据和选举时投票 
- 不推荐使用 Arbiter(投票节点)

### 数据是如何复制的?
- 当一个修改操作，无论是插入、更新或删除，到达主节点时，它对数据的操作将被 记录下来(经过一些必要的转换)，这些记录称为 oplog。
- 从节点通过在主节点上打开一个 tailable 游标不断获取新进入主节点的 oplog，并 在自己的数据上回放，以此保持跟主节点的数据一致。

### 通过选举完成故障恢复
- 具有投票权的节点之间两两互相发送心跳;
- 当5次心跳未收到时判断为节点失联;
- 如果失联的是主节点，从节点会发起选举，选出新的主节点;
- 如果失联的是从节点则不会产生新的选举;
- 选举基于 RAFT一致性算法 实现，选举成功的必要条件是大多数投票节点存活;
- $\color{red}{复制集中最多可以有50个节点，但具有投票权的节点最多7个}$

### 影响选举的因素
- 整个集群必须有大多数节点存活;
- 被选举为主节点的节点必须:
- 能够与多数节点建立连接
- 具有较新的 oplog
- 具有较高的优先级(如果有配置)

### 复制集节点有以下常见的选配项:
- 是否具有投票权(v 参数):有则参与投票;
- 优先级(priority 参数):优先级越高的节点越优先成为主节点。优先级为0的节点无法成 为主节点;
- 隐藏(hidden 参数):复制数据，但对应用不可见。隐藏节点可以具有投票仅，但优先 级必须为0;
- 延迟(slaveDelay 参数):复制 n 秒之前的数据，保持与主节点的时间差。

### $\color{red}{增加复制节点不会增加系统写性能}$
--- 
- 配置mongo配置文件
```yaml
# mongod.conf
systemLog:
  destination: file
  path: /data1/mongod.log   # 日志文件路径
  logAppend: true
storage:
  dbPath: /data1    # 数据目录
net:
  bindIp: 0.0.0.0
  port: 28017   # 端口
replication:
  replSetName: rs0
processManagement:
  fork: true 
```

- 创建复制集
```javascript
// 方法1，注意:此方式hostname 需要能被解析
# mongo --port 28017
> rs.initiate()
> rs.add(”HOSTNAME:28018") 
> rs.add(”HOSTNAME:28019")

// ---------------------------------------------------
// 方法2
# mongo --port 28017
> rs.initiate({
    _id: "rs0",
    members: [{
        _id: 0,
        host: "primary:27017"
    },{
        _id: 1,
        host: "secondary1:27017"
    },{
        _id: 2,
        host: "secondary2:27017"
    }]
})
```

# mongo 分片集群
### 为什么要使用分片集群?
- 数据容量日益增大，访问性能日渐降低，怎么破? 
- 新品上线异常火爆，如何支撑更多的并发用户?
- 单库已有 10TB 数据，恢复需要1-2天，如何加速? 
- 地理分布数据
![分片集群](https://s1.ax1x.com/2020/10/10/0yIp3q.png)
- mongos路由节点
  - 提供集群单一入口
  - 转发应用端请求
  - 选择合适数据节点进行读写
  - 合并多个数据节点的返回
  - 无状态 建议至少2个
- 配置节点mongod
  - 配置(目录)节点
  - 提供集群元数据存储
  - 分片数据分布的映射
  - 普通复制集架构
- 数据节点mongod
  - 以复制集为单位 
  - 横向扩展 
  - 最大1024分片 
  - 分片之间数据不重复 
  - 所有分片在一起才可完整工作

### 分片集群数据分布方式
- 基于范围
- 基于 Hash
- 基于 zone / tag

### 分片集群数据分布方式 – 基于范围
![分片集群](https://s1.ax1x.com/2020/10/10/0yoxlq.png)

### 分片集群数据分布方式 – 基于哈希
![分片集群](https://s1.ax1x.com/2020/10/10/0yTe6x.png)

### 分片集群数据分布方式 – 自定义Zone
![分片集群](https://s1.ax1x.com/2020/10/10/0yTtjP.png)

### 分片大小
- 分片的基本标准:
  - 关于数据:数据量不超过3TB，尽可能保持在2TB一个片; -
  - 关于索引:常用索引必须容纳进内存;
- 按照以上标准初步确定分片后，还需要考虑业务压力，随着压力增大，CPU、RAM、 磁盘中的任何一项出现瓶颈时，都可以通过添加更多分片来解决。

### 合理的架构 – 需要多少个分片
- A = 所需存储总量 / 单服务器可挂载容量 8TB / 2TB = 4
- B = 工作集大小 / 单服务器内存容量 400GB / (256G * 0.6) = 3
- C = 并发量总数 / (单服务器并发量 * 0.7)[额外开销] 30000 / (9000*0.7) = 6
- 分片数量 = max(A, B, C) = 6

### 分片关键字
- 片键 shard key:文档中的一个字段
- 文档 doc :包含 shard key 的一行数据
- 块 Chunk :包含 n 个文档
- 分片 Shard:包含 n 个 chunk
- 集群 Cluster: 包含 n 个分片
![分片集群](https://s1.ax1x.com/2020/10/10/0yHMJH.png)

### 选择基数大的片键
- 对于小基数的片键:
  - 因为备选值有限，那么块的总数量就有限; 
  - 随着数据增多，块的大小会越来越大;
  - 水平扩展时移动块会非常困难;
- 例如:存储一个高中的师生数据，以年龄(假设年龄范围为15~65岁)作为片键， 那么:
  - 15<=年龄<=65，且只为整数 
  - 最多只会有51个chunk

### 选择分布均匀的片键
- 对于分布不均匀的片键:
  - 造成某些块的数据量急剧增大
  - 这些块压力随之增大
  - 数据均衡以 chunk 为单位，所以系统无能为力
- 例如:存储一个学校的师生数据，以年龄(假设年龄范围为15~65岁)作为片键， 那么:
  - 15<=年龄<=65，且只为整数
  - 大部分人的年龄范围为15~18岁(学生)
  - 15、16、17、18四个 chunk 的数据量、访问压力远大于其他 chunk

### 判断当前运行的Mongo服务是否为主节点可以使用命令db.isMaster()

### 配置mongo允许从节点读数据
```javascript
# mongo localhost:28018
> rs.slaveOk()
```